# Copy this file to .env and fill in secrets
# Environment variables used by the project

# =====================
# Flask API settings
# =====================
# Optional API key; when set, clients must send header: X-API-Key: <value>
# API_KEY=
# Limit payload size (bytes); default 1MB
# MAX_CONTENT_LENGTH=1048576
# Rate limiter storage backend (use Redis/Memcached in production to avoid memory storage):
# RATE_LIMIT_STORAGE_URI=redis://localhost:6379

# Optional build/version metadata surfaced by /version
# APP_VERSION=0.1.0
# GIT_COMMIT=dev

# =====================
# Sentry error tracking
# =====================
# SENTRY_DSN=https://<public_key>@o<org>.ingest.sentry.io/<project>
# Sample rates (0.0 to 1.0)
# SENTRY_TRACES_SAMPLE_RATE=0.1
# SENTRY_PROFILES_SAMPLE_RATE=0.0

# Path to trained model artifact (optional; defaults to models/legal_case_classifier.pkl)
# MODEL_PATH=models/legal_case_classifier.pkl

# Path to semantic search index (optional; defaults to models/search_index.pkl)
# SEARCH_INDEX_PATH=models/search_index.pkl

# =====================
# Confidence & Abstention Settings
# =====================
# Predictions below this threshold are flagged for review (0.0-1.0)
CONFIDENCE_THRESHOLD=0.5
# Automatically add low-confidence predictions to active learning queue
AUTO_QUEUE_LOW_CONFIDENCE=1

# =====================
# Explainability Settings
# =====================
# Number of key factors to return in prediction response
EXPLAIN_TOP_K=5

# =====================
# Memory Optimization (for Render Free Tier 512MB)
# =====================
# Enable low-memory mode (disables optional features)
LOW_MEMORY=1
# Maximum search results to return
MAX_SEARCH_RESULTS=10
# Lazy load search index on first request (saves startup memory)
LAZY_LOAD_SEARCH=0
# Disable indices for minimum memory footprint
DISABLE_SEARCH_INDEX=0
DISABLE_SEMANTIC_INDEX=1

# =====================
# Gunicorn Settings (Production)
# =====================
# Keep low for free tier (512MB RAM)
GUNICORN_WORKERS=1
GUNICORN_THREADS=2

# =====================
# Training Options
# =====================
# Enable SMOTE oversampling for minority classes (training only)
ENABLE_SMOTE=0
# Target sampling ratio for minority classes
SMOTE_SAMPLING_STRATEGY=0.5

###########################################
# Hugging Face enrichment (optional)
###########################################
# Use local extractive summarization instead of HF API (recommended)
USE_LOCAL_SUMMARY=1
# API token to enable calling HF Inference API for summarization
# HUGGINGFACE_API_TOKEN=hf_xxx
# Optionally point to a specific summarization model endpoint
# HUGGINGFACE_API_URL=https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6
# Disable HF calls (use local fallbacks)
HUGGINGFACE_DISABLE=1
# Rate limit sleep (seconds) between HF calls
# HF_RATE_LIMIT_SLEEP=0.3

# Harvesting controls for scripts/kanoon_harvest.py (optional)
# KANOON_PAGES=10
# KANOON_QUERIES_FILE=data/queries.csv
